{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c542eb-5a78-4016-9c52-7af81500f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#%set_env PYTORCH_ROCM_ARCH=\"gfx1100\"\n",
    "#%set_env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "#%set_env HIP_VISIBLE_DEVICES=0\n",
    "#%set_env ROCM_PATH=/opt/rocm\n",
    "#%set_env HIP_LAUNCH_BLOCKING=1\n",
    "%set_env AMD_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0e31333-4592-4e31-8c5f-d870c94815ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "manual_seed = 9999\n",
    "# manual_seed = random.randint(1, 10000)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.use_deterministic_algorithms(True) # for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1714e8ab-6f6d-4941-8d9d-5d051ff6ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"data/celeba\"\n",
    "workers = 2\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3                   # num of color channels\n",
    "nz = 100                 # len of latent vector (size of generator input)\n",
    "ngf = 64                 # size of feature maps in generator\n",
    "ndf = 64                 # size of feature maps in discriminator\n",
    "num_epochs = 5\n",
    "lr = 0.0002              # learning rate\n",
    "beta1 = 0.5              # beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "# num of GPUs (0 == run in CPU mode)\n",
    "ngpu = 1 if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa23349f-7bcf-4802-92c6-74b92f605e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD Radeon RX 6700 XT\n",
      "cuda:0\n",
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing HIP_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mtranspose(vutils\u001b[38;5;241m.\u001b[39mmake_grid(\u001b[43mreal_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m64\u001b[39m],\n\u001b[1;32m     26\u001b[0m                                          padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     27\u001b[0m                                          normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     28\u001b[0m                         (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing HIP_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=workers)\n",
    "\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "#print(torch.rand(3, 3).to(device))\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64],\n",
    "                                         padding=2,\n",
    "                                         normalize=True).cpu(),\n",
    "                        (1,2,0)))\n",
    "\n",
    "#plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9501ae-1319-4c40-94f5-53e635146c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37394c-945d-4573-9f4f-252cfc5a258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            #  input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfae62-a408-4363-a718-a3d05e132ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# For multi-GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly\n",
    "# apply all weights to ``mean=0``, ``stdev=0.02``.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe98ccd-680e-439c-80d2-ec762fdbfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is ``(nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67f0a8-97bf-4026-8f2e-cbea01f460a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# For multi-GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallell(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly\n",
    "# initialize all weights like this:\n",
    "# ``to mean=0, stdev=0.2``.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aea68a-359c-4879-835f-eebd5496bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create a batch of latent vectors that we will use to\n",
    "# visualize the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e04bc-df79-4568-be4e-04627c8e1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "            1) Update D network:\n",
    "            maximize log(D(x)) + log(1 - D(G(z))\n",
    "        \"\"\"\n",
    "        ## train w/ all-real batch\n",
    "        netD.zero_grad()\n",
    "        # format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,),\n",
    "                           real_label,\n",
    "                           dtype=torch.float,\n",
    "                           device=device\n",
    "                          )\n",
    "        # format pass real batch thru D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calc loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calc gradients for D in backwards pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## train w/ all-fake batch\n",
    "        # Generate batch of latent noise\n",
    "        noise = torch.randn(b_size,\n",
    "                            nz, \n",
    "                            1,\n",
    "                            1,\n",
    "                            device=device\n",
    "                           )\n",
    "        # Generate fake img batch w/ G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch w/ D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch,\n",
    "        # accumulated w/ prev gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake\n",
    "        # and real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        \"\"\"\n",
    "            2) Update G Network\n",
    "            maximize log(D(G(z)))\n",
    "        \"\"\"\n",
    "        netG.zero_grad()\n",
    "        # real labels are real for generator cost\n",
    "        label.fill_(real_label)\n",
    "        # since we just updated D, \n",
    "        # perform another forward pass of all-fake\n",
    "        # batch thru D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calc G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calc gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLossG: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by \n",
    "        # saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs - 1) \\\n",
    "                                  and (i == len(dataloader) - 1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
